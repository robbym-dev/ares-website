{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"getting_started.html","title":"Introduction","text":"<p>Welcome to the Automated Evaluation Framework for Retrieval-Augmented Generation Systems (ARES). ARES is a groundbreaking framework for evaluating Retrieval-Augmented Generation (RAG) models. The automated process combines synthetic data generation with fine-tuned classifiers to efficiently assess context relevance, answer faithfulness, and answer relevance, minimizing the need for extensive human annotations. ARES employs synthetic query generation and Precision-Performance Iteration (PPI), providing accurate evaluations with statistical confidence. </p> <p> </p>"},{"location":"getting_started.html#get-started","title":"Get Started","text":"\u2699\ufe0f Installation <p>Set up ARES efficiently with our step-by-step installation guide.</p> \ud83d\udcaa  Synthetic Generation <p>Discover how to automatically create synthetic datasets that closely mimic real-world scenarios for robust RAG testing.</p> \ud83d\udcca Training Classifier <p>Learn how to train high-precision classifiers to determine the relevance and faithfulness of RAG outputs</p> \u2699\ufe0f RAG Evaluation <p>Configure RAG model evaluation with ARES to accurately evaluate your model's performance.</p>"},{"location":"installation.html","title":"Installation","text":"<p>To install the necessary dependencies, run the following commands: \u200b</p> <pre><code>pip install ares-ai\n</code></pre> <p>If you would like to directly install from Github repo, run the following commands: </p> <pre><code>git clone https://github.com/stanford-futuredata/ARES.git\ncd ARES\npip install -e .\n</code></pre> <p>Optional: Initalize OpenAI or TogetherAI API key with the following commands:</p> <pre><code>export OPENAI_API_KEY=&lt;your key here&gt;\n</code></pre> <pre><code>export TOGETHER_API_KEY=&lt;your key here&gt;\n</code></pre>"},{"location":"quick_local_model_execution.html","title":"Local Model Execution","text":""},{"location":"quick_local_model_execution.html#local-model-execution-with-vllm","title":"Local Model Execution with vLLM","text":"<p>ARES supports vLLM, allowing for local execution of LLM models, offering enhanced privacy and the ability to operate ARES offline. Below are steps to use vLLM for with ARES's UES/IDP and PPI!</p>"},{"location":"quick_local_model_execution.html#uesidp-w-vllm","title":"UES/IDP w/ vLLM","text":"<pre><code>from ares import ARES\n\nues_idp_config = {\n    \"in_domain_prompts_dataset\": \"nq_few_shot_prompt_for_judge_scoring.tsv\",\n    \"unlabeled_evaluation_set\": \"nq_unlabeled_output.tsv\", \n    \"model_choice\": \"meta-llama/Llama-2-13b-hf\", # Specify vLLM model\n    \"vllm\": True, # Toggle vLLM to True \n    \"host_url\": \"http://0.0.0.0:8000/v1\" # Replace with server hosting model followed by \"/v1\"\n} \n\nares = ARES(ues_idp=ues_idp_config)\nresults = ares.ues_idp()\nprint(results)\n</code></pre>"},{"location":"quick_local_model_execution.html#ppi-w-vllm","title":"PPI w/ vLLM","text":"<pre><code>from ares import ARES\n\nppi_config = { \n    \"evaluation_datasets\": ['nq_unabeled_output.tsv'], \n    \"few_shot_examples_filepath\": \"nq_few_shot_prompt_for_judge_scoring.tsv\",\n    \"llm_judge\": \"meta-llama/Llama-2-13b-hf\", # Specify vLLM model\n    \"labels\": [\"Context_Relevance_Label\"], \n    \"gold_label_path\": \"nq_labeled_output.tsv\",\n    \"vllm\": True, # Toggle vLLM to True \n    \"host_url\": \"http://0.0.0.0:8000/v1\" # Replace with server hosting model followed by \"/v1\"\n}\n\nares = ARES(ppi=ppi_config)\nresults = ares.evaluate_RAG()\nprint(results)\n</code></pre> <p>For more details, refer to our documentation.</p> <p></p>"},{"location":"quick_start_guide_1.html","title":"Quick Start: Guide A","text":""},{"location":"quick_start_guide_1.html#overview","title":"Overview","text":"<p>Required Datasets</p> <p>Ensure you have installed all necessary datasets from the setup page here</p> <p>This quick start guide demonstrates how to run an evaluation using a large language model (LLM) (in this case, GPT-3.5) on an unlabeled evaluation set with in-domain prompting. We will compare the results of running the evaluation with the LLM model alone and in conjunction with ARES's Prediction Powered Inference (PPI).</p> <p>By following this guide, you will see how ARES's PPI significantly enhances the performance and accuracy of the evaluation. Just copy-paste as you go to see ARES in action! Below is an example of a configuration for ARES:</p>"},{"location":"quick_start_guide_1.html#step-1-run-the-following-to-retrive-the-uesidp-scores-with-gpt35","title":"Step 1) Run the following to retrive the UES/IDP scores with GPT3.5!","text":"<pre><code>from ares import ARES\n\nues_idp_config = {\n    \"in_domain_prompts_dataset\": \"nq_few_shot_prompt_for_judge_scoring.tsv\",\n    \"unlabeled_evaluation_set\": \"nq_unlabeled_output.tsv\", \n    \"model_choice\" : \"gpt-3.5-turbo-0125\"\n} \n\nares = ARES(ues_idp=ues_idp_config)\nresults = ares.ues_idp()\nprint(results)\n# {'Context Relevance Scores': [Score], 'Answer Faithfulness Scores': [Score], 'Answer Relevance Scores': [Score]}\n</code></pre>"},{"location":"quick_start_guide_1.html#step-2-run-the-following-to-retrieve-aress-ppi-scores-with-gpt35","title":"Step 2) Run the following to retrieve ARES's PPI scores with GPT3.5!","text":"<pre><code>ppi_config = { \n    \"evaluation_datasets\": ['nq_unlabeled_output.tsv'], \n    \"few_shot_examples_filepath\": \"nq_few_shot_prompt_for_judge_scoring.tsv\",\n    \"llm_judge\": \"gpt-3.5-turbo-1106\",\n    \"labels\": [\"Context_Relevance_Label\"], \n    \"gold_label_path\": \"nq_labeled_output.tsv\", \n}\n\nares = ARES(ppi=ppi_config)\nresults = ares.evaluate_RAG()\nprint(results)\n</code></pre>"},{"location":"quick_start_guide_2.html","title":"Quick Start: Guide B","text":""},{"location":"quick_start_guide_2.html#overview","title":"Overview","text":"<p>Required Datasets</p> <p>Ensure you have installed all necessary datasets from the setup page here</p> <p>This quick start guide demonstrates how to use a large language model (LLM), specifically GPT-3.5, to evaluate an unlabeled dataset with in-domain prompting. Additionally, it showcases ARES's robust process, which includes synthetic data generation, training a classifier, and using Prediction Powered Inference (PPI) to significantly enhance evaluation accuracy.</p>"},{"location":"quick_start_guide_2.html#step-1-run-the-following-to-see-gpt-35s-accuracy-on-the-nq-unlabeled-dataset","title":"Step 1) Run the following to see GPT 3.5's accuracy on the NQ unlabeled dataset!","text":"<pre><code>from ares import ARES\n\nues_idp_config = {\n    \"in_domain_prompts_dataset\": \"nq_few_shot_prompt_for_judge_scoring.tsv\",\n    \"unlabeled_evaluation_set\": \"nq_unlabeled_output.tsv\", \n    \"model_choice\" : \"gpt-3.5-turbo-0125\"\n} \n\nares = ARES(ues_idp=ues_idp_config)\nresults = ares.ues_idp()\nprint(results)\n# {'Context Relevance Scores': [Score], 'Answer Faithfulness Scores': [Score], 'Answer Relevance Scores': [Score]}\n</code></pre>"},{"location":"quick_start_guide_2.html#step-2-run-the-following-to-see-aress-synthetic-generation-in-action","title":"Step 2) Run the following to see ARES's synthetic generation in action!","text":"<pre><code>\nfrom ares import ARES\n\nsynth_config = { \n    \"document_filepaths\": [\"nq_labeled_output.tsv\"] ,\n    \"few_shot_prompt_filename\": \"nq_few_shot_prompt_for_synthetic_query_generation.tsv\",\n    \"synthetic_queries_filenames\": [\"synthetic_queries_1.tsv\"], \n    \"documents_sampled\": 6189\n}\n\nares_module = ARES(synthetic_query_generator=synth_config)\nresults = ares_module.generate_synthetic_data()\nprint(results)\n</code></pre>"},{"location":"quick_start_guide_2.html#step-3-run-the-following-to-see-aress-training-classifier-in-action","title":"Step 3) Run the following to see ARES's training classifier in action!","text":"<pre><code>\nfrom ares import ARES\n\nclassifier_config = {\n    \"training_dataset\": [\"synthetic_queries_1.tsv\"], \n    \"validation_set\": [\"nq_labeled_output.tsv\"], \n    \"label_column\": [\"Context_Relevance_Label\"], \n    \"num_epochs\": 10, \n    \"patience_value\": 3, \n    \"learning_rate\": 5e-6,\n    \"assigned_batch_size\": 1,  \n    \"gradient_accumulation_multiplier\": 32,  \n}\n\nares = ARES(classifier_model=classifier_config)\nresults = ares.train_classifier()\nprint(results)\n</code></pre> <p>Note: This code creates a checkpoint for the trained classifier. Training may take some time. You can download our jointly trained checkpoint on context relevance here! Download Checkpoint</p> <p>Alternatively, you can download our jointly trained checkpoint on answer relevance as well! Be sure to change the parameters in the config to match the label \"Answer_Relevance_Label\" Download Checkpoint </p>"},{"location":"quick_start_guide_2.html#step-4-run-the-following-to-see-aress-ppi-in-action","title":"Step 4) Run the following to see ARES's PPI in action!","text":"<pre><code>\nfrom ares import ARES\n\nppi_config = { \n    \"evaluation_datasets\": ['nq_unlabeled_output.tsv'], \n    \"few_shot_examples_filepath\": \"nq_few_shot_prompt_for_judge_scoring.tsv\",\n    \"checkpoints\": [\"Context_Relevance_Label_nq_labeled_output_date_time.pt\"], \n    \"rag_type\": \"question_answering\", \n    \"labels\": [\"Context_Relevance_Label\"], \n    \"gold_label_path\": \"nq_labeled_output.tsv\", \n}\n\nares = ARES(ppi=ppi_config)\nresults = ares.evaluate_RAG()\nprint(results)\n</code></pre>"},{"location":"rag_eval.html","title":"RAG Evaluation Starter Guide","text":"This page teaches you how to configure the RAG model evaluation with ARES to accurately evaluate your model's performance."},{"location":"rag_eval.html#configure-api-key","title":"Configure API Key.","text":"<pre><code>export OPENAI_API_KEY=&lt;your key here&gt;\nexport TOGETHER_API_KEY=&lt;your key here&gt;\nexport ANTHROPIC_API_KEY=&lt;your key here&gt;\n</code></pre>"},{"location":"rag_eval.html#rag-evaluation-configuration","title":"RAG Evaluation Configuration","text":"<p>The synth_config dictionary is a configuration object that sets up ARES for generating synthetic queries based on a given dataset. Below is how the synthetic generation configuration style.</p> <pre><code>from ares import ARES\n\nppi_config = { \n    \"evaluation_datasets\": [&lt;eval_dataset_filepath&gt;],\n    \"few_shot_examples_filepath\": &lt;few_shot_filepath&gt;,\n    \"checkpoints\": [&lt;checkpoint_filepath&gt;],\n    \"labels\": [&lt;labels&gt;], \n    \"model_choice\": &lt;model_choice&gt;, # Default model is \"microsoft/deberta-v3-large\"\n    \"gold_label_path\": &lt;gold_label_filepath&gt;\n}\n\nares_module = ARES(ppi=ppi_config)\nresults = ares_module.evaluate_RAG()\nprint(results)\n</code></pre>"},{"location":"rag_eval.html#evaluation-datasets","title":"Evaluation Dataset(s)","text":"<p>Input file paths to datasets for PPI evaluation, which should contain labeled data for validating classifier performance.</p> <pre><code>\"evaluation_datasets\": [\"nq_unlabeled_output.tsv\"],\n</code></pre> <p>Link to ARES Setup for evaluation dataset example file used. </p>"},{"location":"rag_eval.html#few-shot-prompt-file-path","title":"Few-Shot Prompt File Path","text":"<p>Specify the file path for a file with few-shot examples, which PPI uses to understand the labeling schema and guide the evaluation.</p> <pre><code>\"few_shot_prompt_filename\": \"data/datasets/multirc_few_shot_prompt_for_synthetic_query_generation_v1.tsv\",\n</code></pre> <p>Link to ARES Setup for few-shot file example used. </p>"},{"location":"rag_eval.html#checkpoints-file-path","title":"Checkpoint(s) File Path","text":"<p>Generated from ARES Training Classifier, provide file path(s) to model checkpoint file(s), representing the saved states of the trained classifiers used for evaluation.</p> <pre><code>\"checkpoints\": [\"output/checkpoint_generated_from_training_classifier\"],\n</code></pre>"},{"location":"rag_eval.html#labels","title":"Labels","text":"<p>List the names of label columns or individual label column in your dataset(s) that PPI will use for evaluation metrics.</p> <pre><code>\"labels\": [\"Context_Relevance_Label\"], \n</code></pre>"},{"location":"rag_eval.html#gold-label-path","title":"Gold Label Path","text":"<pre><code>\"gold_label_path\": \"nq_labeled_output.tsv\"\n</code></pre> <p>Link to ARES Setup for gold label path file example used. </p>"},{"location":"rag_eval.html#rag-evaluation-configuration-full-example","title":"RAG Evaluation Configuration: Full Example","text":"<pre><code>from ares import ARES\n\nppi_config = { \n    \"evaluation_datasets\": ['nq_ratio_0.6.tsv'], \n    \"few_shot_examples_filepath\": \"nq_few_shot_prompt_for_judge_scoring.tsv\",\n    \"checkpoints\": [\"/future/u/manihani/ARES/checkpoints/microsoft-deberta-v3-large/Context_Relevance_Label_joint_datasets_2024-04-30_01:01:01.pt\"], \n    \"labels\": [\"Context_Relevance_Label\"], \n    \"gold_label_path\": \"nq_labeled_output.tsv\"\n}\n\nares = ARES(ppi=ppi_config)\nresults = ares.evaluate_RAG()\nprint(results)\n\n</code></pre> <p>Download the necessary files for this example here!</p>"},{"location":"rag_eval_params.html","title":"Capabilities and Parameters","text":"This page teaches you how to configure the RAG model evaluation with ARES to accurately evaluate your model's performance."},{"location":"rag_eval_params.html#configure-api-key","title":"Configure API Key.","text":"<pre><code>export OPENAI_API_KEY=&lt;your key here&gt;\nexport TOGETHER_API_KEY=&lt;your key here&gt;\nexport ANTHROPIC_API_KEY=&lt;your key here&gt;\n</code></pre>"},{"location":"rag_eval_params.html#rag-evaluation-configuration","title":"RAG Evaluation Configuration","text":"<p>The synth_config dictionary is a configuration object that sets up ARES for generating synthetic queries based on a given dataset. Below is how the synthetic generation configuration style.</p> <pre><code>from ares import ARES\n\nppi_config = { \n    \"evaluation_datasets\": [&lt;eval_dataset_filepath&gt;],\n    \"few_shot_examples_filepath\": &lt;few_shot_filepath&gt;,\n    \"checkpoints\": [&lt;checkpoint_filepath&gt;],\n    \"labels\": [&lt;labels&gt;], \n    \"model_choice\": &lt;model_choice&gt;, \n    \"gold_label_path\": &lt;gold_label_filepath&gt;, \n    \"model_choice\": \"microsoft/deberta-v3-large\",\n    \"llm_judge\": \"None\",\n    \"assigned_batch_size\": 1,\n    \"number_of_labels\": 2,\n    \"alpha\": 0.05,\n    \"num_trials\": 1000,\n    \"vllm\": False,\n    \"host_url\": \"http://0.0.0.0:8000/v1\",\n    \"request_delay\": 0,\n    \"debug_mode\": False,\n    \"machine_label_llm_model\": \"None\",\n    \"gold_machine_label_path\": \"None\"\n}\n\nares_module = ARES(ppi=ppi_config)\nresults = ares_module.evaluate_RAG()\nprint(results)\n</code></pre>"},{"location":"rag_eval_params.html#evaluation-datasets","title":"Evaluation Dataset(s)","text":"<p>Input file paths to datasets for PPI evaluation, which should contain labeled data for validating classifier performance.</p> <pre><code>\"evaluation_datasets\": [\"nq_unlabeled_output.tsv\"],\n</code></pre> <p>Link to ARES Setup for evaluation dataset example file used. </p>"},{"location":"rag_eval_params.html#few-shot-prompt-file-path","title":"Few-Shot Prompt File Path","text":"<p>Specify the file path for a file with few-shot examples, which PPI uses to understand the labeling schema and guide the evaluation.</p> <pre><code>\"few_shot_prompt_filename\": \"data/datasets/multirc_few_shot_prompt_for_synthetic_query_generation_v1.tsv\",\n</code></pre> <p>Link to ARES Setup for few-shot file example used. </p>"},{"location":"rag_eval_params.html#checkpoints-file-path","title":"Checkpoint(s) File Path","text":"<p>Generated from ARES Training Classifier, provide file path(s) to model checkpoint file(s), representing the saved states of the trained classifiers used for evaluation.</p> <pre><code>\"checkpoints\": [\"output/checkpoint_generated_from_training_classifier\"],\n</code></pre>"},{"location":"rag_eval_params.html#labels","title":"Labels","text":"<p>List the names of label columns or individual label column in your dataset(s) that PPI will use for evaluation metrics.</p> <pre><code>\"labels\": [\"Context_Relevance_Label\"], \n</code></pre>"},{"location":"rag_eval_params.html#gold-label-path","title":"Gold Label Path","text":"<p>Specify the file path for the gold label dataset, which contains the true labels for the evaluation dataset. This file is used as a reference to measure the performance of the classifier by comparing its predictions against these true labels. </p> <pre><code>\"gold_label_path\": \"nq_labeled_output.tsv\"\n</code></pre> <p>Link to ARES Setup for gold label path file example used. </p>"},{"location":"rag_eval_params.html#model-choice","title":"Model Choice","text":"<p>The name of the pre-trained model to be used for evaluation. In this example, microsoft/deberta-v3-large is chosen for its strong performance in text understanding and classification tasks. You can replace this with other model names based on your requirements and the specific tasks you are working on.</p> <pre><code>\"model_choice\": \"microsoft/deberta-v3-large\"\n</code></pre>"},{"location":"rag_eval_params.html#llm-judge","title":"LLM Judge","text":"<p>Specify the name of the LLM model to be used for evaluation. This is an optional parameter and can be set to \"None\" if no LLM model is needed for evaluation.</p> <pre><code>\"llm_judge\": \"None\"\n</code></pre> <p>Notice</p> <p>Only specify LLM judge if you do not provide a checkpoint, if both are specified then the checkpoint will be used.</p>"},{"location":"rag_eval_params.html#assigned-batch-size","title":"Assigned Batch Size","text":"<p>Determines the number of samples processed in each batch during evaluation. Smaller batch sizes can lead to more frequent updates but might be slower due to less parallel processing. Larger batch sizes can speed up the process but require more memory.</p> <pre><code>\"assigned_batch_size\": 1\n</code></pre>"},{"location":"rag_eval_params.html#number-of-labels","title":"Number of Labels","text":"<p>Specifies the number of distinct labels used for classification tasks. This is crucial for setting up the model's output layer correctly and for interpreting the evaluation results.</p> <pre><code>\"number_of_labels\": 2\n</code></pre>"},{"location":"rag_eval_params.html#alpha","title":"Alpha","text":"<p>Represents the significance level used in statistical hypothesis testing. It defines the threshold for rejecting the null hypothesis, with common values being 0.05, 0.01, etc.</p> <pre><code>\"alpha\": 0.05\n</code></pre>"},{"location":"rag_eval_params.html#num-trials","title":"Num Trials","text":"<p>Specifies the number of trials or iterations used to estimate confidence intervals and other statistics utilized in PPI. Higher values can improve the accuracy of the estimates but require more computational resources.</p> <pre><code>\"num_trials\": 1000\n</code></pre>"},{"location":"rag_eval_params.html#vllm","title":"vLLM","text":"<p>A flag to determine whether to use the vLLM API for evaluation. Setting this to True enables the use of vLLM.</p> <pre><code>\"vllm\": False\n</code></pre>"},{"location":"rag_eval_params.html#host-url","title":"Host URL","text":"<p>Specifies the host URL for the LLM API. This is an optional parameter and can be set to \"http://0.0.0.0:8000/v1\" if the LLM API is running locally.</p> <pre><code>\"host_url\": \"http://0.0.0.0:8000/v1\"\n</code></pre> <p>Notice</p> <p>If you are using vLLM, ensure that the host URL is correct and the LLM API is running.</p>"},{"location":"rag_eval_params.html#request-delay","title":"Request Delay","text":"<p>Specifies the delay in seconds between each request to the LLM API. This is an optional parameter and can be set to 0 if no delay is needed.</p> <pre><code>\"request_delay\": 0\n</code></pre>"},{"location":"rag_eval_params.html#debug-mode","title":"Debug Mode","text":"<p>A flag to determine whether to run the evaluation in debug mode. This is an optional parameter and can be set to False if debug mode is not needed.</p> <pre><code>\"debug_mode\": False\n</code></pre>"},{"location":"rag_eval_params.html#machine-label-llm-model","title":"Machine Label LLM Model","text":"<p>The machine_label_llm_model parameter specifies the LLM model to be used for generating machine labels. This can be useful for automated labeling processes in the absence of a gold label path.</p> <pre><code>\"machine_label_llm_model\": \"None\"\n</code></pre>"},{"location":"rag_eval_params.html#gold-machine-label-path","title":"Gold Machine Label Path","text":"<p>The file path to the machine-generated gold labels. By default is set to \"None\" if not using machine-generated gold labels.</p> <pre><code>\"gold_machine_label_path\": \"None\"\n</code></pre> <p>Notice</p> <p>Specify gold_machine_label_path if you are using a machine_label_llm_model.</p>"},{"location":"rag_eval_params.html#rag-evaluation-configuration-full-example","title":"RAG Evaluation Configuration: Full Example","text":"<pre><code>from ares import ARES\n\nppi_config = { \n    \"evaluation_datasets\": ['nq_ratio_0.6.tsv'], \n    \"few_shot_examples_filepath\": \"nq_few_shot_prompt_for_judge_scoring.tsv\",\n    \"checkpoints\": [\"/future/u/manihani/ARES/checkpoints/microsoft-deberta-v3-large/Context_Relevance_Label_joint_datasets_2024-04-30_01:01:01.pt\"], \n    \"labels\": [\"Context_Relevance_Label\"], \n    \"gold_label_path\": \"nq_labeled_output.tsv\",\n    \"model_choice\": \"microsoft/deberta-v3-large\",\n    \"llm_judge\": \"None\",\n    \"assigned_batch_size\": 1,\n    \"number_of_labels\": 2,\n    \"alpha\": 0.05,\n    \"num_trials\": 1000,\n    \"vllm\": False,\n    \"host_url\": \"http://0.0.0.0:8000/v1\",\n    \"request_delay\": 0,\n    \"debug_mode\": False,\n    \"machine_label_llm_model\": \"None\",\n    \"gold_machine_label_path\": \"None\"\n}\n\nares = ARES(ppi=ppi_config)\nresults = ares.evaluate_RAG()\nprint(results)\n\n</code></pre> <p>Download the necessary files for this example here!</p>"},{"location":"setup.html","title":"Setup","text":""},{"location":"setup.html#requirements","title":"Requirements","text":"<p>To implement ARES for scoring your RAG system and comparing to other RAG configurations, you need three components:\u200b</p> <ul> <li>A human preference validation set of annotated query, document, and answer triples for the evaluation criteria (e.g. context relevance, answer faithfulness, and/or answer relevance). There should be at least 50 examples but several hundred examples is ideal.</li> <li>A set of few-shot examples for scoring context relevance, answer faithfulness, and/or answer relevance in your system</li> <li>A much larger set of unlabeled query-document-answer triples outputted by your RAG system for scoring</li> </ul> <p></p> <p>To get started with ARES, you'll need to set up your configuration. Below is an example of a configuration for ARES!</p> <p>Copy-paste each step to see ARES in action!</p>"},{"location":"setup.html#download-datasets","title":"Download datasets","text":"<p>Use the following command to quickly obtain the necessary files for getting started! This includes the 'few_shot_prompt' file for judge scoring and synthetic query generation, as well as both labeled and unlabeled datasets.</p> <pre><code>wget https://raw.githubusercontent.com/stanford-futuredata/ARES/main/datasets/example_files/nq_few_shot_prompt_for_judge_scoring.tsv\nwget https://raw.githubusercontent.com/stanford-futuredata/ARES/main/datasets/example_files/nq_few_shot_prompt_for_synthetic_query_generation.tsv\nwget https://raw.githubusercontent.com/stanford-futuredata/ARES/main/datasets/example_files/nq_labeled_output.tsv\nwget https://raw.githubusercontent.com/stanford-futuredata/ARES/main/datasets/example_files/nq_unlabeled_output.tsv\n</code></pre> <p>OPTIONAL: You can run the following command to get the full NQ dataset! (347 MB)</p> <pre><code>from ares import ARES\nares = ARES() \nares.KILT_dataset(\"nq\")\n\n# Fetches NQ datasets with ratios including 0.5, 0.6, 0.7, etc.\n# For purposes of our quick start guide, we rename nq_ratio_0.5 to nq_unlabeled_output and nq_labeled_output.\n</code></pre>"},{"location":"synth_gen.html","title":"Synthetic Generation Starter Guide","text":"This page shows you how to automatically create synthetic datasets that closely mimic real-world scenarios for robust RAG testing."},{"location":"synth_gen.html#synth-gen-configuration","title":"Synth Gen Configuration","text":"<p>The synth_config dictionary is a configuration object that sets up ARES for generating synthetic queries based on a given dataset. Below is how the synthetic generation configuration style.</p> <pre><code>from ares import ARES\n\nsynth_config = { \n    \"document_filepaths\": [&lt;document_filepaths&gt;],\n    \"few_shot_prompt_filename\": few_shot_filepath,\n    \"synthetic_queries_filenames\": [&lt;synthetic_queries_filepaths&gt;],\n    \"model_choice\": &lt;model_choice&gt;,\n    \"documents_sampled\": 10000\n}\n\nares = ARES(synthetic_query_generator=synth_config)\nresults = ares.generate_synthetic_data()\nprint(results)\n</code></pre>"},{"location":"synth_gen.html#document-file-paths","title":"Document File Path(s)","text":"<p>A single or list of file paths to the document(s) you want to use for generating synthetic queries. If given a list of file paths, each file path should point to a file containing raw text from which ARES can derive context for the synthetic queries. </p> <pre><code>\"document_filepaths\": [\"/data/datasets_v2/nq/nq_ratio_0.5_.tsv\"], \n</code></pre> <p>Link to ARES Github Repo for document example file used. </p>"},{"location":"synth_gen.html#few-shot-prompt-file-path","title":"Few-Shot Prompt File Path","text":"<p>This refers to the file paths for a few-shot prompt file that provide examples of queries and answers for ARES to learn from. Few-shot learning uses a small amount of labeled training data to guide the generation of synthetic queries.</p> <pre><code>\"few_shot_prompt_filename\": \"data/datasets/multirc_few_shot_prompt_for_synthetic_query_generation_v1.tsv\",\n</code></pre> <p>Link to ARES Github Repo for few-shot file example used. </p>"},{"location":"synth_gen.html#synthetic-queries-filepath","title":"Synthetic Queries Filepath","text":"<p>A list of file paths where the generated synthetic queries will be saved. These files will store the queries created by ARES for use in training or evaluation. </p> <p>NOTE - List Size Verification</p> <p>Ensure the synthetic queries file paths list matches the document file paths list in size for consistency.</p> <pre><code>\"synthetic_queries_filenames\": [\"/output/synthetic_queries_1.tsv\"],\n</code></pre>"},{"location":"synth_gen.html#model-choice","title":"Model Choice","text":"<p>Specifies the pre-trained language model to create the synthetic data. By default, ARES uses \"google/flan-t5-xxl\". You can replace this with any Hugging Face model suitable for your task.</p> <pre><code> \"model_choice\": \"google/flan-t5-xxl\",\n</code></pre>"},{"location":"synth_gen.html#documents-sampled","title":"Documents Sampled","text":"<p>An integer indicating how many documents to sample from your dataset when generating synthetic queries. Sampling can help speed up processing and manage computational resources. Choose a value that represents a large enough sample to generate meaningful synthetic queries, but not so large as to make processing infeasible. ARES will automatically filter documents</p> <p>NOTE - Document Filter</p> <p>ARES will automatically filter documents less than 50 words</p> <pre><code>\"documents_sampled\": 10000,\n</code></pre>"},{"location":"synth_gen.html#synthetic-generation-configuration-full-example","title":"Synthetic Generation Configuration: Full Example","text":"<pre><code>from ares import ARES\n\nsynth_config = { \n    \"document_filepaths\": [\"/data/datasets_v2/nq/nq_ratio_0.5_.tsv\"],\n    \"few_shot_prompt_filename\": \"data/datasets/multirc_few_shot_prompt_for_synthetic_query_generation_v1.tsv\",\n    \"synthetic_queries_filenames\": [\"/output/synthetic_queries_1.tsv\"],\n    \"model_choice\": \"google/flan-t5-xxl\",\n    \"documents_sampled\": 10000\n}\n\nares = ARES(synthetic_query_generator=synth_config)\nresults = ares.generate_synthetic_data()\nprint(results)\n</code></pre>"},{"location":"synth_gen_params.html","title":"Capabilities and Parameters","text":"This page provides an in-depth overview of the parameters and capabilities available for synthetic data generation in ARES, allowing users to fully customize their datasets for robust testing and evaluation."},{"location":"synth_gen_params.html#synthetic-generation-in-depth-configuration","title":"Synthetic Generation In-Depth Configuration","text":"<p>The synth_config dictionary is a configuration object that sets up ARES for generating synthetic queries based on a given dataset. Below is how the synthetic generation configuration style.</p> <p>Synthetic Generation Parameters</p> <p>Inherently, in ARES the values past <code>documents_sampled</code> are not required and will use the default values if not provided.</p> <pre><code>from ares import ARES\n\nsynth_config = { \n    \"document_filepaths\": [&lt;document_filepaths&gt;],\n    \"few_shot_prompt_filename\": few_shot_filepath,\n    \"synthetic_queries_filenames\": [&lt;synthetic_queries_filepaths&gt;],\n    \"model_choice\": &lt;model_choice&gt;,\n    \"documents_sampled\": 10000,\n    \"model_choice\": \"google/flan-t5-xxl\", \n    \"clean_documents\": False, \n    \"regenerate_synth_questions\": True, \n    \"percentiles\": 0.05, 0.25, 0.5, 0.95,\n    \"question_temperatures\": 2.0, 1.5, 1.0, 0.5, 0.0,\n    \"regenerate_answers\": True, \n    \"number_of_negatives_added_ratio\": 0.5,\n    \"lower_bound_for_negatives\": 5,\n    \"number_of_contradictory_answers_added_ratio\": 0.67,\n    \"number_of_positives_added_ratio\": 0.0,\n    \"regenerate_embeddings\": True,\n    \"synthetic_query_prompt\": \"You are an expert question-answering system. \n    You must create a question for the provided document. \n    The question must be answerable within the context of the document.\\n\\n\"\n}\n\nares = ARES(synthetic_query_generator=synth_config)\nresults = ares.generate_synthetic_data()\nprint(results)\n</code></pre>"},{"location":"synth_gen_params.html#document-file-paths","title":"Document File Path(s)","text":"<p>A single or list of file paths to the document(s) you want to use for generating synthetic queries. If given a list of file paths, each file path should point to a file containing raw text from which ARES can derive context for the synthetic queries. </p> <pre><code>\"document_filepaths\": [\"/data/datasets_v2/nq/nq_ratio_0.5_.tsv\"], \n</code></pre> <p>Link to ARES Github Repo for document example file used. </p>"},{"location":"synth_gen_params.html#few-shot-prompt-file-path","title":"Few-Shot Prompt File Path","text":"<p>This refers to the file paths for a few-shot prompt file that provide examples of queries and answers for ARES to learn from. Few-shot learning uses a small amount of labeled training data to guide the generation of synthetic queries.</p> <pre><code>\"few_shot_prompt_filename\": \"data/datasets/multirc_few_shot_prompt_for_synthetic_query_generation_v1.tsv\",\n</code></pre> <p>Link to ARES Github Repo for few-shot file example used. </p>"},{"location":"synth_gen_params.html#synthetic-queries-filepath","title":"Synthetic Queries Filepath","text":"<p>A list of file paths where the generated synthetic queries will be saved. These files will store the queries created by ARES for use in training or evaluation. </p> <p>NOTE - List Size Verification</p> <p>Ensure the synthetic queries file paths list matches the document file paths list in size for consistency.</p> <pre><code>\"synthetic_queries_filenames\": [\"/output/synthetic_queries_1.tsv\"],\n</code></pre>"},{"location":"synth_gen_params.html#model-choice","title":"Model Choice","text":"<p>Specifies the pre-trained language model to create the synthetic data. By default, ARES uses \"google/flan-t5-xxl\". You can replace this with any Hugging Face model suitable for your task.</p> <pre><code> \"model_choice\": \"google/flan-t5-xxl\",\n</code></pre>"},{"location":"synth_gen_params.html#documents-sampled","title":"Documents Sampled","text":"<p>An integer indicating how many documents to sample from your dataset when generating synthetic queries. Sampling can help speed up processing and manage computational resources. Choose a value that represents a large enough sample to generate meaningful synthetic queries, but not so large as to make processing infeasible. ARES will automatically filter documents</p> <p>NOTE - Document Filter</p> <p>ARES will automatically filter documents less than 50 words</p> <pre><code>\"documents_sampled\": 10000,\n</code></pre>"},{"location":"synth_gen_params.html#clean-documents","title":"Clean Documents","text":"<p>A boolean indicating whether to clean the documents before generating synthetic queries. This is useful if the documents contain special characters or are in a different language.</p> <pre><code>\"clean_documents\": True,\n</code></pre>"},{"location":"synth_gen_params.html#regenerate-synth-questions","title":"Regenerate Synth Questions","text":"<p>A boolean indicating whether to regenerate synthetic questions for each document. This is useful if the synthetic questions are not satisfactory.</p> <pre><code>\"regenerate_synth_questions\": True,\n</code></pre>"},{"location":"synth_gen_params.html#percentiles","title":"Percentiles","text":"<p>A list of floats indicating the percentiles of the synthetic questions to generate. The percentiles should be between 0 and 1.</p> <pre><code>\"percentiles\": [0.05, 0.25, 0.5, 0.95],\n</code></pre>"},{"location":"synth_gen_params.html#question-temperatures","title":"Question Temperatures","text":"<p>A list of floats indicating the temperatures of the synthetic questions to generate. The temperatures should be between 0 and 2.</p> <pre><code>\"question_temperatures\": [2.0, 1.5, 1.0, 0.5, 0.0],\n</code></pre>"},{"location":"synth_gen_params.html#regenerate-answers","title":"Regenerate Answers","text":"<p>A boolean indicating whether to regenerate synthetic answers for each document. This is useful if the synthetic answers are not satisfactory.</p> <pre><code>\"regenerate_answers\": True,\n</code></pre>"},{"location":"synth_gen_params.html#number-of-negatives-added-ratio","title":"Number of Negatives Added Ratio","text":"<p>A float indicating the ratio of synthetic queries to generate that are negatives. The ratio should be between 0 and 1.</p> <pre><code>\"number_of_negatives_added_ratio\": 0.5,\n</code></pre>"},{"location":"synth_gen_params.html#number-of-contradictory-answers-added-ratio","title":"Number of Contradictory Answers Added Ratio","text":"<p>A float indicating the ratio of synthetic queries to generate that are contradictory answers. The ratio should be between 0 and 1.</p> <pre><code>\"number_of_contradictory_answers_added_ratio\": 0.67,\n</code></pre>"},{"location":"synth_gen_params.html#number-of-positives-added-ratio","title":"Number of Positives Added Ratio","text":"<p>A float indicating the ratio of synthetic queries to generate that are positives. The ratio should be between 0 and 1.</p> <pre><code>\"number_of_positives_added_ratio\": 0.0,\n</code></pre>"},{"location":"synth_gen_params.html#regenerate-embeddings","title":"Regenerate Embeddings","text":"<p>A boolean indicating whether to regenerate embeddings for each document. This is useful if the embeddings are not satisfactory.</p> <pre><code>\"regenerate_embeddings\": True,\n</code></pre>"},{"location":"synth_gen_params.html#synthetic-query-prompt","title":"Synthetic Query Prompt","text":"<p>A string indicating the prompt for generating synthetic queries. The prompt should be a clear and concise explanation of the task and the format of the synthetic queries.</p> <pre><code>\"synthetic_query_prompt\": \"You are an expert question-answering system. \n    You must create a question for the provided document. \n    The question must be answerable within the context of the document.\\n\\n\",\n</code></pre> <p>Prompt Engineering</p> <p>Proceed with caution when modifying the prompt, the data generation process is crucial to ARES's performance.</p>"},{"location":"training_classifier.html","title":"Training Classifier Starter Guide","text":"This pages teach you how to train high-precision classifiers to determine the relevance and faithfulness of RAG outputs"},{"location":"training_classifier.html#training-classifier-configuration","title":"Training Classifier Configuration","text":"<p>The synth_config dictionary is a configuration object that sets up ARES for generating synthetic queries based on a given dataset. Below is how the training classifier configuration style.</p> <pre><code>from ares import ARES\n\nclassifier_config = {\n    \"classification_dataset\": [&lt;classification_dataset_filepath&gt;],\n    \"test_set_selection\": &lt;test_set_selection_filepath&gt;, \n    \"label_column\": [&lt;labels&gt;], \n    \"model_choice\": \"microsoft/deberta-v3-large\", # Default model is \"microsoft/deberta-v3-large\"\n    \"num_epochs\": 10, \n    \"patience_value\": 3, \n    \"learning_rate\": 5e-6\n}\n\nares = ARES(classifier_model=classifier_config)\nresults = ares.train_classifier()\nprint(results)\n\n</code></pre>"},{"location":"training_classifier.html#classification-dataset","title":"Classification Dataset","text":"<p>Generated from the ARES synthetic generator, here you should provide a list of file paths or an individual filepath to your labeled dataset used for training the classifier. The dataset should include text data and corresponding labels for supervised learning.</p> <pre><code>\"classification_dataset\": [\"output/synthetic_queries_1.tsv\"],\n</code></pre>"},{"location":"training_classifier.html#test-set-selection","title":"Test Set Selection","text":"<p>Provide the file path to your test set for evaluating the classifier's performance. This should be separate from the training data to ensure an unbiased assessment.</p> <pre><code>\"test_set_selection\": \"/data/datasets_v2/nq/nq_ratio_0.6_.tsv\"\n</code></pre> <p>Link to ARES Github Repo for test set selection file example used. </p>"},{"location":"training_classifier.html#label-columns","title":"Label Column(s)","text":"<p>List the column name(s) in your dataset that contain the label(s). These are the targets your classifier will predict.</p> <pre><code>\"label_column\": [\"Conmtext_Relevance_Label\"], \n</code></pre>"},{"location":"training_classifier.html#model-choice","title":"Model Choice","text":"<p>Specifies the pre-trained language model to fine-tune for classification. By default, ARES uses \"microsoft/deberta-v3-large\". You can replace this with any Hugging Face model suitable for your task.</p> <pre><code> \"model_choice\": \"google/flan-t5-xxl\",\n</code></pre>"},{"location":"training_classifier.html#num-epochs","title":"Num Epochs","text":"<p>Determines the number of training epochs, which is the number of times the learning algorithm will work through the entire training dataset.</p> <pre><code>\"num_epochs\": 10, \n</code></pre>"},{"location":"training_classifier.html#patience-value","title":"Patience Value","text":"<p>This is used in early stopping to prevent overfitting. It's the number of epochs with no improvement on the validation set after which training will be stopped.</p> <pre><code>\"patience_value\": 3, \n</code></pre>"},{"location":"training_classifier.html#learning-rate","title":"Learning Rate","text":"<p>Sets the initial learning rate for the optimizer. This is a crucial hyperparameter that controls the adjustment of model weights during training. </p> <pre><code> \"learning_rate\": 5e-6\n</code></pre>"},{"location":"training_classifier.html#training-classifier-configuration-full-example","title":"Training Classifier Configuration: Full Example","text":"<pre><code>from ares import ARES\n\nclassifier_config = {\n    \"classification_dataset\": [\"output/synthetic_queries_1.tsv\"], \n    \"validation_set\": \"./datasets_v2/nq/ratio_0.5_reformatted_full_articles_False_validation_with_negatives.tsv\",\n    \"label_column\": [\"Context_Relevance_Label\"], \n    \"model_choice\": \"microsoft/deberta-v3-large\",\n    \"num_epochs\": 10, \n    \"patience_value\": 3, \n    \"learning_rate\": 5e-6\n}\n\nares = ARES(classifier_model=classifier_config)\nresults = ares.train_classifier()\nprint(results)\n</code></pre>"},{"location":"training_classifier_params.html","title":"Capabilities and Parameters","text":"This page provides an in-depth overview of the parameters and capabilities available for the training classifier in ARES, allowing users to fully customize the training pipeline in ARES."},{"location":"training_classifier_params.html#training-classifier-configuration","title":"Training Classifier Configuration","text":"<p>The synth_config dictionary is a configuration object that sets up ARES for generating synthetic queries based on a given dataset. Below is how the training classifier configuration style.</p> <p>Training Classifier Parameters</p> <p>Inherently, in ARES the values past <code>learning_rate</code> are not required and will use the default values if not provided. Review values '''assigned_batch_size''' and '''gradient_accumulation_multiplier''', they are dependent on your system.</p> <pre><code>\n    classifier_config = {\n    \"classification_dataset\": [&lt;classification_dataset_filepath&gt;],\n    \"validation_set\": &lt;test_set_selection_filepath&gt;, \n    \"label_column\": [&lt;labels&gt;], \n    \"model_choice\": \"microsoft/deberta-v3-large\", \n    \"num_epochs\": 10, \n    \"patience_value\": 3, \n    \"learning_rate\": 5e-6,\n    \"training_dataset_path\": \"path/to/training/dataset.tsv\",\n    \"validation_dataset_path\":  \"path/to/validation/dataset.tsv\",\n    \"validation_set_scoring\": True,\n    \"assigned_batch_size\": 1,\n    \"gradient_accumulation_multiplier\": 32,\n    \"number_of_runs\": 1,\n    \"num_warmup_steps\": 100,\n    \"training_row_limit\": -1,\n    \"validation_row_limit\": -1\n}\n\n</code></pre>"},{"location":"training_classifier_params.html#classification-dataset","title":"Classification Dataset","text":"<p>Generated from the ARES synthetic generator, here you should provide a list of file paths or an individual filepath to your labeled dataset used for training the classifier. The dataset should include text data and corresponding labels for supervised learning.</p> <pre><code>\"classification_dataset\": [\"output/synthetic_queries_1.tsv\"],\n</code></pre> <p># of Training Datasets</p> <p>Ensure the number of training datasets provided aligns with number of validation datasets.</p>"},{"location":"training_classifier_params.html#validation-set","title":"Validation Set","text":"<p>Provide the file path to your validation set for evaluating the classifier's performance. This should be separate from the training data to ensure an unbiased assessment.</p> <pre><code>\"validation_set\": \"/data/datasets_v2/nq/nq_ratio_0.6_.tsv\"\n</code></pre> <p># of Training Datasets</p> <p>Ensure the number of validation datasets provided aligns with number of training datasets.</p> <p>Link to ARES Github Repo for test set selection file example used. </p>"},{"location":"training_classifier_params.html#label-columns","title":"Label Column(s)","text":"<p>List the column name(s) in your dataset that contain the label(s). These are the targets your classifier will predict.</p> <pre><code>\"label_column\": [\"Conmtext_Relevance_Label\"], \n</code></pre>"},{"location":"training_classifier_params.html#model-choice","title":"Model Choice","text":"<p>Specifies the pre-trained language model to fine-tune for classification. By default, ARES uses \"microsoft/deberta-v3-large\". You can replace this with any Hugging Face model suitable for your task.</p> <pre><code> \"model_choice\": \"google/flan-t5-xxl\",\n</code></pre>"},{"location":"training_classifier_params.html#num-epochs","title":"Num Epochs","text":"<p>Determines the number of training epochs, which is the number of times the learning algorithm will work through the entire training dataset.</p> <pre><code>\"num_epochs\": 10, \n</code></pre>"},{"location":"training_classifier_params.html#patience-value","title":"Patience Value","text":"<p>This is used in early stopping to prevent overfitting. It's the number of epochs with no improvement on the validation set after which training will be stopped.</p> <pre><code>\"patience_value\": 3, \n</code></pre>"},{"location":"training_classifier_params.html#learning-rate","title":"Learning Rate","text":"<p>Sets the initial learning rate for the optimizer. This is a crucial hyperparameter that controls the adjustment of model weights during training. </p> <pre><code> \"learning_rate\": 5e-6\n</code></pre>"},{"location":"training_classifier_params.html#training-dataset-path","title":"Training Dataset Path","text":"<p>If more than 1 training dataset is provided, the classifier will combine all the datasets into one dataset path and train on all of them. In this case, please provide path to save the combined training dataset.</p> <pre><code>\"training_dataset_path\": \"path/to/training/dataset.tsv\"\n</code></pre>"},{"location":"training_classifier_params.html#validation-dataset-path","title":"Validation Dataset Path","text":"<p>If more than 1 validation dataset is provided, the classifier will combine all the datasets into one dataset path and utilize all of them for validation. In this case, please provide path to save the combined validation dataset.</p> <pre><code>\"validation_dataset_path\": \"path/to/validation/dataset.tsv\"\n</code></pre>"},{"location":"training_classifier_params.html#validation-set-scoring","title":"Validation Set Scoring","text":"<p>If True, the classifier will evaluate the model on the validation set after each epoch. If False, the classifier will only evaluate the model on the test set after the final epoch.</p> <pre><code>\"validation_set_scoring\": True,\n</code></pre>"},{"location":"training_classifier_params.html#assigned-batch-size","title":"Assigned Batch Size","text":"<p>The batch size for training. This is a crucial hyperparameter that controls the number of samples processed in each iteration.</p> <pre><code>\"assigned_batch_size\": 1,\n</code></pre>"},{"location":"training_classifier_params.html#gradient-accumulation-multiplier","title":"Gradient Accumulation Multiplier","text":"<p>The number of steps to accumulate the gradients before performing a backward pass. This is a crucial hyperparameter that controls the number of steps to accumulate the gradients before performing a backward pass.</p> <pre><code>\"gradient_accumulation_multiplier\": 32,\n</code></pre>"},{"location":"training_classifier_params.html#number-of-runs","title":"Number of Runs","text":"<p>The number of times to run the training process. This is a crucial hyperparameter that controls the number of times to run the training process.</p> <pre><code>\"number_of_runs\": 1,\n</code></pre>"},{"location":"training_classifier_params.html#num-warmup-steps","title":"Num Warmup Steps","text":"<p>The number of steps to warm up the learning rate. This is a crucial hyperparameter that controls the number of steps to warm up the learning rate.</p> <pre><code>\"num_warmup_steps\": 100,\n</code></pre>"},{"location":"training_classifier_params.html#training-row-limit","title":"Training Row Limit","text":"<p>The number of rows to limit the training dataset to. This is a crucial hyperparameter that controls the number of rows to limit the training dataset to.</p> <pre><code>\"training_row_limit\": -1,\n</code></pre>"},{"location":"training_classifier_params.html#validation-row-limit","title":"Validation Row Limit","text":"<p>The number of rows to limit the validation dataset to. This is a crucial hyperparameter that controls the number of rows to limit the validation dataset to.</p> <pre><code>\"validation_row_limit\": -1,\n</code></pre>"}]}